project:
  name: "fine_tune_research"

dataset:
  type: "s3"
  name: "fine-tuning-research/mmlu"
  format: "completion"

model:
  base:
    type: "hf"
    name: "meta-llama/Llama-2-7b-hf"
  output:
    type: "hf"
    name: "rparundekar/llama2-7b-mmlu"

training:
  trainer:
    packing: False
    max_seq_length: 512
  sft:
    per_device_train_batch_size: 24
    per_device_eval_batch_size: 24
    fp16: False
    learning_rate: 0.0002
    lr_scheduler_type: "cosine"
    warmup_ratio: 0.1
    num_train_epochs: 1
    gradient_accumulation_steps: 4
    gradient_checkpointing: True
    gradient_checkpointing_kwargs:
      use_reentrant: False
    logging_strategy: "steps"
    logging_steps: 5
    evaluation_strategy: "steps"
    eval_steps: 10
  other:
    freeze_embed: True
    n_freeze: 24
